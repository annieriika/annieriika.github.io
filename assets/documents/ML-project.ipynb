{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Comparing Naive Bayes and Neural Networks in Text Categorization of the Brown Corpus\n",
    "\n",
    "The task I am trying to solve is text categorization of the Brown corpus. Previously on the course we classified the sentiment of movie reviews using both Naive Bayes algorithm and neural networks. In those tasks, a review was detected as either positive or negative. What makes this task different, is that there are multiple possible categories that can be assigned to the given text. I want to see how well both Naive Bayes and neural networks can solve this task. I also want to learn to implement neural networks on my own data, since we did not get to dive very deep on that on this course.\n",
    "\n",
    "To accurately compare the models, both models will use supervised machine learning and a bag of words approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to do data partitioning and preprocessing it is essential to know what the data consists of. The data is imported from the NLTK Gutenberg corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure       69342\n",
      "belles_lettres  173096\n",
      "editorial       61604\n",
      "fiction         68488\n",
      "government      70117\n",
      "hobbies         82345\n",
      "humor           21695\n",
      "learned         181888\n",
      "lore            110299\n",
      "mystery         57169\n",
      "news            100554\n",
      "religion        39399\n",
      "reviews         40704\n",
      "romance         70022\n",
      "science_fiction 14470\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg, brown\n",
    "\n",
    "# Print categories and number of words in category\n",
    "for category in brown.categories():\n",
    "    print('{:<16}{}'.format(category, len(brown.words(categories=category))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Brown corpus consists of over a million words compiled from 500 samples from 15 different genres. As can be seen in the output of the code above, the words are not evenly divided between categories, and some categories contain considerably more words than other categories. Therefore, some categories will be more represented in the training data. However, all categories should be somewhat equally represented in the test set. The training and test set will consist of 90% and 10% of the data respectively. A validation/development set will not be used for the Naive Bayes algorithm, because I will be using the same features to accurately compare Naive Bayes and neural networks and therefore not doing any feature engineering.\n",
    "\n",
    "Because the data is labelled, the models will be trained using supervised machine learning, where the model learns the connections between the data and its labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in the test set:  50\n",
      "Samples in the train set:  450\n"
     ]
    }
   ],
   "source": [
    "# Create a list of texts and categories\n",
    "documents = [(list(brown.words(fileid)), category)\n",
    "            for category in brown.categories()\n",
    "            for fileid in brown.fileids(category)]\n",
    "\n",
    "# Take every 10th element from the documents for the test set\n",
    "test_set = documents[4::10] # Starting at 4th element results in every genre being included in the test set\n",
    "print(\"Samples in the test set: \",len(test_set))\n",
    "\n",
    "train_set = [d for d in documents if d not in test_set]\n",
    "print(\"Samples in the train set: \",len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I took 10% of the corpus for a test set by taking every 10th sample from the documents. This way, I ended up with 50 samples for testing and remaining 450 for training. I manually checked the distribution of categories, and ended up starting from the 4th document for a distribution that picks a sample even from the smallest category. Next, I will check the distribution of categories in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Categories and amount of samples in the dataset-- \n",
      "\n",
      "adventure          29\n",
      "belles_lettres     75\n",
      "editorial          27\n",
      "fiction            29\n",
      "government         30\n",
      "hobbies            36\n",
      "humor               9\n",
      "learned            80\n",
      "lore               48\n",
      "mystery            24\n",
      "news               44\n",
      "religion           17\n",
      "reviews            17\n",
      "romance            29\n",
      "science_fiction     6\n",
      "dtype: int64 \n",
      "\n",
      "--Categories and amount of samples in the test set-- \n",
      "\n",
      "adventure          3\n",
      "belles_lettres     7\n",
      "editorial          3\n",
      "fiction            3\n",
      "government         3\n",
      "hobbies            4\n",
      "humor              1\n",
      "learned            8\n",
      "lore               4\n",
      "mystery            3\n",
      "news               4\n",
      "religion           2\n",
      "reviews            2\n",
      "romance            2\n",
      "science_fiction    1\n",
      "dtype: int64 \n",
      "\n",
      "Percentage of each category in test set:  [0.1  0.09 0.11 0.1  0.1  0.11 0.11 0.1  0.08 0.12 0.09 0.12 0.12 0.07\n",
      " 0.17]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a list of categories\n",
    "categories = []\n",
    "for d, c in documents:\n",
    "    categories.append(c)\n",
    "\n",
    "# Count number of samples in each category using pandas\n",
    "n_categories = pd.value_counts(np.array(categories)).sort_index()\n",
    "print('--Categories and amount of samples in the dataset--', '\\n')\n",
    "print(n_categories, '\\n')\n",
    "\n",
    "# Create a list of categories in the test set\n",
    "test_categories = []\n",
    "for d, c in test_set:\n",
    "    test_categories.append(c)\n",
    "    \n",
    "# Count number of samples in each category using pandas\n",
    "n_test_categories = pd.value_counts(np.array(test_categories)).sort_index()\n",
    "print('--Categories and amount of samples in the test set--', '\\n')\n",
    "print(n_test_categories, '\\n')\n",
    "\n",
    "# Count the percentage of each category in test set\n",
    "percentage = n_test_categories.values / n_categories.values \n",
    "print(\"Percentage of each category in test set: \", percentage.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every genre is represented in the test set, and as can be seen from the output of the code, about 10% of each category is taken for the test set. The smallest category is slightly overrepresented with 17%, but that is only 1 sample out of 6, and I wanted to include each category in the test set, to see whether the algorithms could assign a label even to the smallest category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the datasets are partitioned, I will implement the Naive Bayes classifier. First, word features need to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining most frequent words\n",
    "all_words = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "word_features = [w for w, f in all_words.most_common(2000)]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Featuresets\n",
    "train_features = [(document_features(d), c) for (d,c) in train_set]\n",
    "test_features = [(document_features(d), c) for (d,c) in test_set] \n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2000 most frequent words in the Brown corpus are taken as word features. Then the feature extractor is defined, and the classifier simply checks whether the text contains a certain word or not, and decides which category the text belongs to. This makes it a bag of words model, where the order of the words is not taken into consideration. This type of model is suitable for this dataset, where the samples consist of quite large text documents of over 2000 words each. The words the documents contain already give out a lot of useful information for recognizing the categories, because intuitively each category contains similar content, and therefore the classifier will be able to assign labels based on word features.\n",
    "\n",
    "Now everything is set to train and evaluate the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy:  0.6\n",
      "Most Informative Features\n",
      "        contains(didn't) = True           myster : learne =     45.3 : 1.0\n",
      "      contains(wouldn't) = True           myster : learne =     34.3 : 1.0\n",
      "        contains(walked) = True           myster : learne =     32.1 : 1.0\n",
      "        contains(wasn't) = True           romanc : learne =     30.4 : 1.0\n",
      "           contains(sat) = True           fictio : learne =     29.7 : 1.0\n",
      "       contains(stopped) = True           romanc : learne =     28.7 : 1.0\n",
      "      contains(watching) = True           romanc : learne =     28.7 : 1.0\n",
      "        contains(killed) = True           scienc : learne =     28.4 : 1.0\n",
      "         contains(sleep) = True           scienc : learne =     28.4 : 1.0\n",
      "       contains(watched) = True           scienc : learne =     28.4 : 1.0\n",
      "     contains(telephone) = True            humor : belles =     28.1 : 1.0\n",
      "           contains(ran) = True           advent : learne =     27.9 : 1.0\n",
      "        contains(season) = True           review : belles =     27.3 : 1.0\n",
      "        contains(mother) = True           romanc : learne =     26.9 : 1.0\n",
      "        contains(smiled) = True           scienc : belles =     26.8 : 1.0\n",
      "     contains(somewhere) = True           scienc : belles =     26.8 : 1.0\n",
      "       contains(dropped) = True           advent : learne =     26.1 : 1.0\n",
      "        contains(street) = True           myster : learne =     25.4 : 1.0\n",
      "       contains(waiting) = True           myster : learne =     25.4 : 1.0\n",
      "           contains(car) = True            humor : learne =     24.3 : 1.0\n",
      "        contains(dinner) = True            humor : learne =     24.3 : 1.0\n",
      "        contains(waited) = True           myster : learne =     23.2 : 1.0\n",
      "        contains(you're) = True           myster : learne =     23.2 : 1.0\n",
      "       contains(meeting) = True            humor : belles =     23.0 : 1.0\n",
      "     contains(beautiful) = True           review : learne =     22.8 : 1.0\n",
      "         contains(music) = True           review : learne =     22.8 : 1.0\n",
      "          contains(baby) = True           romanc : learne =     21.7 : 1.0\n",
      "          contains(desk) = True           myster : learne =     21.0 : 1.0\n",
      "          contains(road) = True           myster : learne =     21.0 : 1.0\n",
      "          contains(neck) = True           advent : learne =     20.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Naive Bayes accuracy: \", nltk.classify.accuracy(classifier, test_features))\n",
    "print(classifier.show_most_informative_features(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the model is 60% which is significantly better compared to the naive baseline, i.e. assigning the most common category, learned, to all texts, which would get 8/50 of the genres correct or an accuracy of 16%.\n",
    "\n",
    "Looking at the most informative features, they seem to make sense even just by knowing the labels of different genres: \"killed\" belongs to science fiction, \"ran\" belongs to adventure, \"beautiful\" to review and \"baby\" to romance.\n",
    "\n",
    "Next I will look at the predicted labels and further analyse the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure  :  adventure\n",
      "adventure  :  adventure\n",
      "adventure  :  adventure\n",
      "belles_lettres  :  belles_lettres\n",
      "belles_lettres  :  learned\n",
      "belles_lettres  :  belles_lettres\n",
      "belles_lettres  :  belles_lettres\n",
      "belles_lettres  :  belles_lettres\n",
      "belles_lettres  :  belles_lettres\n",
      "belles_lettres  :  belles_lettres\n",
      "editorial  :  editorial\n",
      "editorial  :  news\n",
      "editorial  :  editorial\n",
      "fiction  :  fiction\n",
      "fiction  :  fiction\n",
      "fiction  :  adventure\n",
      "government  :  learned\n",
      "government  :  hobbies\n",
      "government  :  belles_lettres\n",
      "hobbies  :  news\n",
      "hobbies  :  hobbies\n",
      "hobbies  :  learned\n",
      "hobbies  :  hobbies\n",
      "humor  :  belles_lettres\n",
      "learned  :  hobbies\n",
      "learned  :  learned\n",
      "learned  :  lore\n",
      "learned  :  government\n",
      "learned  :  learned\n",
      "learned  :  hobbies\n",
      "learned  :  learned\n",
      "learned  :  learned\n",
      "lore  :  lore\n",
      "lore  :  lore\n",
      "lore  :  lore\n",
      "lore  :  learned\n",
      "mystery  :  mystery\n",
      "mystery  :  romance\n",
      "mystery  :  romance\n",
      "news  :  news\n",
      "news  :  news\n",
      "news  :  news\n",
      "news  :  news\n",
      "religion  :  belles_lettres\n",
      "religion  :  belles_lettres\n",
      "reviews  :  reviews\n",
      "reviews  :  lore\n",
      "romance  :  romance\n",
      "romance  :  romance\n",
      "science_fiction  :  romance\n"
     ]
    }
   ],
   "source": [
    "# Print correct labels and predicted labels\n",
    "for d, c in test_features:\n",
    "    print(c,' : ' ,classifier.classify(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct label is printed for each sample in the test set, followed by the predicted label. Looking at these, the easiest categories to predict seem to be adventure, belles lettres, fiction, lore, romance and news. Adventure and news were labelled correct every time, and adventure also appears a lot in the most informative features. However, news does not, but it makes sense that it would be a very recognisable genre.\n",
    "\n",
    "The hardest categories to predict appear to be government, learned, mystery, religion, and science fiction. Government, humor, religion and science fiction were never labelled correctly. The category government seems to be hard to predict although not being a small category and the category religion seems to be easily confused with belles lettres, meaning beautiful writing. Smaller categories, like science fiction, are hard to evaluate due to their small sample size. \n",
    "\n",
    "Because the amount of test samples is so small, accuracy of the model can vary a lot depending on the test set. Perhaps a solution to this to consider in future experiments would be to label sentences instead, but that might require a sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will implement neural networks, using Keras from Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 2000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent 2000 words are used as word features like in the Naive Bayes model. Maximum lenght is set to 2000 to achieve documents of equal lenght. Samples in the Brown corpus consist of slightly over 2000 words each. Batch size is set to 32. For this model, a validation set will be used to avoid overfitting.\n",
    "\n",
    "The data is shuffled for random partitioning into training and validation sets. To create a validation set consisting of 10% of the data, 50 samples are taken from the training set. To implement a neural networks model, the data needs to be transformed into document vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (400, 2000)\n",
      "x_val shape: (50, 2000)\n",
      "x_test shape: (50, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle train data\n",
    "import random\n",
    "random.shuffle(train_set)\n",
    "\n",
    "# Define documents\n",
    "x_train = [d for (d,c) in train_set[50:]]\n",
    "x_val = [d for (d,c) in train_set[:50]]\n",
    "x_test = [d for (d,c) in test_set]# Create the tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "t = Tokenizer(num_words = 2000)\n",
    "\n",
    "# Encode documents\n",
    "x_train = t.texts_to_matrix(x_train, mode='binary')\n",
    "x_val = t.texts_to_matrix(x_val, mode='binary')\n",
    "x_test = t.texts_to_matrix(x_test, mode='binary')\n",
    "\n",
    "# Print shape\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I set up a tokenizer so I can use the text to matric function from Keras. It encodes each text sample into a vector. It has multiple modes, out of which the binary mode is similar to the word features in the Naive Bayes model, which checks whether the word occurs in the training data or not. The lenght of the vector is the total size of the vocabulary, which is 2000 words. The first integer of the shape tells the amount of vectors, and the second integer their lenght.\n",
    "\n",
    "Next, the labels, in this case the different categories, need to be preprocessed as well. They need to be in a numerical form, so I will create binary representations of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (400, 21)\n",
      "y_val shape: (50, 21)\n",
      "y_test shape: (50, 21)\n"
     ]
    }
   ],
   "source": [
    "# Define training labels\n",
    "y_train = [c for (d, c) in train_set[50:]]\n",
    "y_val = [c for (d, c) in train_set[:50]]\n",
    "y_test = [c for (d, c) in test_set]\n",
    "\n",
    "# Import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Create MultiLabelBinarizer object\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# One-hot encode data\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_val = mlb.fit_transform(y_val)\n",
    "y_test = mlb.fit_transform(y_test)\n",
    "\n",
    "# Print shape\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MultiLabelBinarizer from scikit-learn is used to create binary representations of the labels. Now everything is set to create and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(21, activation='sigmoid',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sequential model is used because it is a simple neural networks model and therefore fit for a simple classification task. A dropout rate of 0.5 is set to reduce overfitting and sigmoid is used as an activation function. The input shape is defined as 21 based on the shape of the binarized labels. Now, the final step is to train and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 400 samples, validate on 50 samples\n",
      "Epoch 1/8\n",
      "400/400 [==============================] - 1s 2ms/sample - loss: 0.6917 - accuracy: 0.7579 - val_loss: 0.6897 - val_accuracy: 0.7705\n",
      "Epoch 2/8\n",
      "400/400 [==============================] - 0s 191us/sample - loss: 0.6883 - accuracy: 0.7698 - val_loss: 0.6864 - val_accuracy: 0.7686\n",
      "Epoch 3/8\n",
      "400/400 [==============================] - 0s 222us/sample - loss: 0.6849 - accuracy: 0.7714 - val_loss: 0.6831 - val_accuracy: 0.7686\n",
      "Epoch 4/8\n",
      "400/400 [==============================] - 0s 216us/sample - loss: 0.6816 - accuracy: 0.7719 - val_loss: 0.6799 - val_accuracy: 0.7686\n",
      "Epoch 5/8\n",
      "400/400 [==============================] - 0s 223us/sample - loss: 0.6784 - accuracy: 0.7729 - val_loss: 0.6767 - val_accuracy: 0.7686\n",
      "Epoch 6/8\n",
      "400/400 [==============================] - 0s 238us/sample - loss: 0.6752 - accuracy: 0.7729 - val_loss: 0.6736 - val_accuracy: 0.7686\n",
      "Epoch 7/8\n",
      "400/400 [==============================] - 0s 233us/sample - loss: 0.6720 - accuracy: 0.7729 - val_loss: 0.6705 - val_accuracy: 0.7686\n",
      "Epoch 8/8\n",
      "400/400 [==============================] - 0s 261us/sample - loss: 0.6689 - accuracy: 0.7729 - val_loss: 0.6674 - val_accuracy: 0.7686\n",
      "Test Accuracy: 0.7666667\n"
     ]
    }
   ],
   "source": [
    "# Compile and train model\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=8,\n",
    "          validation_data=[x_val, y_val])\n",
    "# Evaluate\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Accuracy:',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy goes up until the 3rd epoch and then stays at 77.4%. The validation accuracy stays at 76.3% throughout all epochs, which may be due to a small validation set. The training loss and validation loss keeps going down, which means that there is a better match between correct labels and predicted labels. Validation accuracy is slightly lower than training accuracy, which means that the model learned from the training data and may have started overfitting, but since validation accuracy does not change, it is hard to say. The test accuracy is 76.7%, which is nearly the same as the validation accuracy. Results might also vary due to different samples in the training, validation and test sets. \n",
    "\n",
    "I tested my hypothesis by changing the sizes of the test and validation sets from 50 or 10% to 100 or 20%. The training accuracy went slightly down to 77.2%, and the validation accuracy went up to 77.1%, which are almost the same, indicating that overfitting has not occured. Test accuracy changed to 76.8%, which is nearly the same as before. Typically, the model is not expected to perform better with a smaller training data, but the changes are very small and therefore probably due to differences in the sets of samples.\n",
    "\n",
    "The test accuracy of 76.7% is significantly better than the Naive Bayes model with an accuracy of 60%. The results show that even a simple neural networks model performs better than a Naive Bayes model constructed in a similar way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, I trained two machine learning models to predict categories in the Brown corpus. First, a Naive Bayes model and then a neural networks model. To allow comparison between the models, both were built similarly using a supervised machine learning method and a bag of words approach. Naive Bayes performed significantly better than the naive baseline with an accuracy of 60%, yet the model fell short from the performance of the neural networks model with an accuracy of 76.6%. The model is very simple, and with some fine tuning, the neural networks model could probably perform better. \n",
    "\n",
    "The results were quite close to what I was expecting, since the task is not as simple as sentiment prediction. In addition, it is not surprising that neural networks performed better than the Naive Bayes classifier, since it is a more advanced algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
